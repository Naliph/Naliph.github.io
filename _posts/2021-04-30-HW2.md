---
layout: post
title: Spectral Clustering
---

A tutorial on a simple version of the *spectral clustering* algorithm for clustering data points.

<br><br>
*Spectral clustering* is an important tool to identify meaningful parts of data sets with complex structure.<br>

To better compare what happen with clustering complex structure without spectral clustering, let's look at an example where we use `sklearn.cluster.KMeans`, a simple clustering tool.

# Example without spectral clustering


```python
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt
```


```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```

![simple-blobs.png](/images/spectral/simple-blobs.png)


*Clustering* refers to the task of separating this data set into the two natural "blobs." K-means is a very common way to achieve this task, which has good performance on circular-ish blobs like these: 


```python
from sklearn.cluster import KMeans

km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![kmean-blobs.png](/images/spectral/kmean-blobs.png)
    


So for simple data blobs, `KMean` clustering works fine. However, on a complex data structure. Let's see how it works:


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```


    
![crescents.png](/images/spectral/crescents.png)
    


The structure now are crescents rather than blobs. The Euclidean coordinates of the data points are contained in the matrix `X`, while the labels of each point are contained in `y`. Now k-means won't work so well, because k-means is, by design, looking for circular clusters. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

    
![kmean-crescents.png](/images/spectral/kmean-crescents.png)
    


The `KMean` completely not working this time! So this is where *spectral clustering* come into place.
<br><br>
Below I will derive and implement spectral clustering for this sort of data structure. Hang tight! :)  

# Spectral Clustering Tutorial


The Euclidean coordinates of the data points are contained in the matrix `X`, while the labels of each point are contained in `y`. 
<br><br>

Let $$d_i = \sum_{j = 1}^n a_{ij}$$ be the $$i$$th row-sum of $$\mathbf{A}$$, which is also called the *degree* of $$i$$. Let $$C_0$$ and $$C_1$$ be two clusters of the data points. We assume that every data point is in either $$C_0$$ or $$C_1$$. The cluster membership as being specified by `y`. We think of `y[i]` as being the label of point `i`. So, if `y[i] = 1`, then point `i` (and therefore row $$i$$ of $$\mathbf{A}$$) is an element of cluster $$C_1$$.

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$
In this expression, 

- $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ is the *cut* of the clusters $$C_0$$ and $$C_1$$. 

- $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the *degree* of row $i$ (the total number of all other rows related to row $$i$$ through $$A$$). The *volume* of cluster $$C_0$$ is a measure of the size of the cluster. 


## *similarity matrix* $$\mathbf{A}$$
- $$\mathbf{A}$$ is a 2D `np.ndarray` matrix with shape `(n, n)` (`n` is the number of data points). 
- Entry `A[i,j]` should be equal to `1` if `X[i]` (the coordinates of data point `i`) is within distance `epsilon` of `X[j]` (the coordinates of data point `j`). 
- To get the distances, use `sklearn.metrics.pairwise_distances` on `X` coordinates. Also, set `epsilon = 0.4` in this part
- The diagonal entries `A[i,i]` equal to zero. Here we are using `np.fill_diagonal()` 

 


```python
from sklearn.metrics import pairwise_distances

# We have the Euclidean coordinate X, label y, with n = 200 above.
# Initiate our similarity matrix size nxn 
A = np.zeros((n,n)).astype(int)
epsilon = 0.4

# pairwise distances matrix from data X
distance_X = pairwise_distances(X)

# Entry A[i,j] = 1 if distance at corresponding distance_X is within epsilon
A[distance_X <= epsilon] = 1

# diagonal entries of A = 0
np.fill_diagonal(A,0)
```


```python
# Check matrix A
A
```




    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 1, 0],
           ...,
           [0, 0, 0, ..., 0, 1, 1],
           [0, 0, 1, ..., 1, 0, 1],
           [0, 0, 0, ..., 1, 1, 0]])



Now we have a matrix that shows which points is nearest to the furthest point (200th). So the lower right corner of the matrix, aside from the diagonal, is filled with 1, indicating those are the closest point within `epsilon` distances from the furthest point. 

## The Cut Term
The cut term $$\mathbf{cut}(C_0, C_1)$$ is the number of nonzero entries in $$\mathbf{A}$$ that relate points in cluster $$C_0$$ to points in cluster $$C_1$$. Saying that this term should be small is the same as saying that points in $$C_0$$ shouldn't usually be very close to points in $$C_1$$. 

`cut(A,y)` function compute the cut term by summing up the entries `A[i,j]` for each pair of points `(i,j)` in different clusters. 

* $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ is the *cut* of the clusters $$C_0$$ and $$C_1$$. 


```python
def cut(A, y):
    # initiate sum for entries A[i,j] for each pair of points (i,j) in y
    cut_sum = 0
    for i in range(n):
        for j in range(n):
            # Check if i and j belong to different cluster, then add ups entries
            if y[i] != y[j]: cut_sum += A[i,j]
    
    return cut_sum    
```

To better understand the cut objective value. We are going to compute the cut term for the true clusters `y` vs. a random vector of random labels `y_random` of length `n` with each label equal to either 0 or 1. 


```python
cut(A,y)
```




    26




```python
# random vector of random labels of length n
y_random = np.random.randint(2, size=n)
cut(A,y_random)
```




    2320



The cut terms reflect total points that are close together but got cut off into different cluster. So, if vector y puts points in matrix $$\mathbf{A}$$ that are close together into different clusters, the cut term will increase. Our target is to minimize this cut terms to better distinguished between spectral clusters. 

`y_random` is randomly assigning labels to points in $$\mathbf{A}$$, resulting in very high cut number. Compare to this random set, the cut objective for the true labels of 26 is *much* smaller, indicating the cut term indeed favors the true clusters over the random ones.



## The Volume Term
Now take a look at the second factor in the norm cut objective. This is the *volume term*. As mentioned above, the *volume* of cluster $$C_0$$ is a measure of how "big" cluster $$C_0$$ is. If we choose cluster $$C_0$$ to be small, then $$\mathbf{vol}(C_0)$$ will be small and $$\frac{1}{\mathbf{vol}(C_0)}$$ will be large, leading to an undesirable higher objective value. 


Function `vols(A,y)` computes the volumes of $$C_0$$ and $$C_1$$, returning them as a tuple. For example, `v0, v1 = vols(A,y)` should result in `v0` holding the volume of cluster `0` and `v1` holding the volume of cluster `1`. 

* $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the *degree* of row $$i$$ (the total number of all other rows related to row $$i$$ through $$A$$). The *volume* of cluster $$C_0$$ is a measure of the size of the cluster. 


```python
def vols(A,y):
    # vols C0 == ð‘– th row-sum of ð€, y label 0, C1 label 1
    vol_C0 = A[y==0,:].sum()
    vol_C1 = A[y==1,:].sum()
    return (vol_C0, vol_C1)
```

## The Normcut Objective
Synthesizing, the binary normcut objective asks us to find clusters $$C_0$$ and $$C_1$$ such that:

1. There are relatively few entries of $$\mathbf{A}$$ that join $$C_0$$ and $$C_1$$. 
2. Neither $$C_0$$ and $$C_1$$ are too small. 



* $$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;$$




```python
def normcut(A,y):
    c = cut(A,y)
    v1, v2 = vols(A,y)
    return c * (1/v1 + 1/v2)
```

compare the `normcut` objective using both the true labels `y` and the fake labels `y_random` generated above. Observe the differences:


```python
print("true labels: ", normcut(A,y))
print("Random labels", normcut(A,y_random))
```

    true labels:  0.02303682466323045
    Random labels 2.0650268054241563
    

The normcut true label cluster is much smaller than the randomized label. Hence, y label of clusters $$C_0, C_1$$ is a good partition of data since $$N_{\mathbf{A}}(C_0, C_1)$$ normcut metric is small.

## Vector z

We have now defined a normalized cut objective which takes small values when the input clusters are (a) joined by relatively few entries in $$A$$ and (b) not too small. One approach to clustering is to try to find a cluster vector `y` such that `normcut(A,y)` is small. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. We need a math trick! 

Here's the trick: define a new vector $$\mathbf{z} \in \mathbb{R}^n$$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$


Note that the signs of  the elements of $$\mathbf{z}$$ contain all the information from $$\mathbf{y}$$: if $$i$$ is in cluster $$C_0$$, then $$y_i = 0$$ and $$z_i > 0$$. 

Next, show that:

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where $$\mathbf{D}$$ is the diagonal matrix with nonzero entries $$d_{ii} = d_i$$, and  where $$d_i = \sum_{j = 1}^n a_i$$ is the degree (row-sum) from before.  

* `transform(A,y)` function computes the appropriate $$\mathbf{z}$$ vector given `A` and `y`, using the formula above. 
* Check the equation above that relates the matrix product to the normcut objective, by computing each side separately and checking that they are equal. 
* Check the identity $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$, where $$\mathbb{1}$$ is the vector of `n` ones (i.e. `np.ones(n)`). This identity effectively says that $$\mathbf{z}$$ should contain roughly as many positive as negative entries. 


#### Transform(A,y)


```python
def transform(A,y):
    v0, v1 = vols(A,y)
    # using the formular for z, set all z values to 1/vol(C0) first
    z = np.full(n, 1/v0)
    
    # then seek out label y=1 and reset the value of z to -1/vol(C1)
    z[y==1] = -1/v1
    
    return z
```

#### Check the 2 normcut objective


```python
# compute D diagonal matrix with nonzero entries dii = di, di = row-sum
D = np.diag(A.sum(axis = 1))

z = transform(A,y)

# check both normcut objectives:
norm1 = normcut(A,y)
norm2 = 2 * (z@(D-A)@z) / (z@D@z)

norm1, norm2
```




    (0.02303682466323045, 0.023036824663230267)



Compare the 2 normcut computation. The equation above is exact, but computer arithmetic is not! Therefore, use`np.isclose(a,b)` to check if `norm1` is equal to `norm2`, in the sense that they differ by less than the smallest amount that the computer is (by default) able to quantify. 


```python
np.isclose(norm1, norm2)
```




    True


#### Check the identity $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$


```python
z@D@np.ones(n), np.isclose(z@D@np.ones(n), 0)
```




    (-4.5102810375396984e-17, True)



This identity effectively proves that $$\mathbf{z}$$ contain roughly as many positive as negative entries. 

## Minimizing the normcut objective, z_min method


In the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. It's actually possible to bake this condition into the optimization, by substituting for $$\mathbf{z}$$ the orthogonal complement of $$\mathbf{z}$$ relative to $$\mathbf{D}\mathbf{1}$$. In the code below, I define an `orth_obj` function which handles this for you. 

Use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $$\mathbf{z}$$. Note that this computation might take a little while. Explicit optimization can be pretty slow! Give the minimizing vector a name `z_min`. 


```python
def orth(u, v):
    return (u @ v) / (v @ v)*v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```


```python
from scipy.optimize import minimize

z_min = minimize(orth_obj, z)
```

**Note**: there's a cheat going on here! We originally specified that the entries of $$\mathbf{z}$$ should take only one of two values (back in Part C), whereas now we're allowing the entries to have *any* value! This means that we are no longer exactly optimizing the normcut objective, but rather an approximation. This cheat is so common that deserves a name: it is called the *continuous relaxation* of the normcut problem. 

#### Plot clusters

Recall that, by design, only the sign of `z_min[i]` actually contains information about the cluster label of data point `i`. Plot the original data, using one color for points such that `z_min[i] < 0` and another color for points such that `z_min[i] >= 0`. 

Due to the approximation, We gonna use `-0.0015` instead of 0


```python
plt.scatter(X[:,0], X[:,1], c = z_min.x < -0.0015)
```

![crescent-cluster-zmin.png](/images/spectral/crescent-cluster-zmin.png))
    


Close! but not quite! 

## Eigenvalues and Eigenvectors, z_eig Method!!!

Explicitly optimizing the orthogonal objective is  *way* too slow to be practical. If spectral clustering required that we do this each time, no one would use it. 

The reason that spectral clustering actually matters, and indeed the reason that spectral clustering is called *spectral* clustering, is that we can actually solve the problem from Part E using eigenvalues and eigenvectors of matrices. 

Recall that what we would like to do is minimize the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

with respect to $$\mathbf{z}$$, subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. 

The Rayleigh-Ritz Theorem states that the minimizing $$\mathbf{z}$$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

which is equivalent to the standard eigenvalue problem 

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

Why is this helpful? Well, $$\mathbb{1}$$ is actually the eigenvector with smallest eigenvalue of the matrix $$\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$. 

> So, the vector $$\mathbf{z}$$ that we want must be the eigenvector with  the *second*-smallest eigenvalue. 

* Construct the matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$, which is often called the *Laplacian* matrix of the similarity matrix $$\mathbf{A}$$. 
* Find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`. 
* plot the data again, using the sign of `z_eig` as the color.


```python
# construct L matrix
L = np.linalg.inv(D)@(D-A)

# eigenvalue and eigenvector
eigval, eigvec = np.linalg.eig((L + L.T)/2)

# argsort eigenvector and correspond eigenvalue from smallest to largest
ix = eigval.argsort()
eigval, eigvec = eigval[ix], eigvec[:,ix]

# second-smallest correspond unit eigenvector
z_eig = eigvec[:,1]

# plot result
plt.scatter(X[:,0], X[:,1], c = z_eig < 0)
```



![crescent-cluster-zeig.png](/images/spectral/crescent-cluster-zeig.png)
    


Much better than the z_min method! In fact, `z_eig` should be proportional to `z_min`, although this won't be exact because minimization has limited precision by default. 

# COMBINE ALL!!!
Synthesize the results from the previous parts in the function `spectral_clustering(X, epsilon)`, which takes in the input data `X` (in the same format as Part A) and the distance threshold `epsilon` and performs spectral clustering, returning an array of binary labels indicating whether data point `i` is in group `0` or group `1`. 


#### Outline

Given data, you need to: 

1. Construct the similarity matrix. 
2. Construct the Laplacian matrix. 
3. Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix. 
4. Return labels based on this eigenvector. 



```python
def spectral_clustering(X, epsilon):
    """
    Performes spectral clustering
    argument:
        X: matrix contain Euclidean coordinates of the data points
        epsilon: distance threshold
    Return:
        an array of binary labels indicating whether data point i is in group 0 or 1.
    """
    
    # construct similarity matrix:
    A = np.zeros((n,n))
    A[pairwise_distances(X) <= epsilon] = 1
    np.fill_diagonal(A,0)
    
    # construct Laplacian matrix:
    D = np.diag(A.sum(axis = 1))
    L = np.linalg.inv(D) @ (D-A)
    
    # compute eigenvector with second-smallest eigenvalue of laplacian matrix:
    eigval, eigvec = np.linalg.eig((L + L.T)/2)
    eigvec = eigvec[:, eigval.argsort()]
    z_eig = eigvec[:,1]
    
    # return labels based on this eigenvector
    return (eigvec[:,1] <0).astype(int)
```


```python
# plot result
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```

![crescent-spectral-cluster.png](/images/spectral/crescent-spectral-cluster.png)



## Examples using `spectral_clustering` function

#### Generate different data sets using `make_moons`. Change `noise` setting:


```python
np.random.seed(1234)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```

![crescent-spectral-cluster-noise1.png](/images/spectral/crescent-spectral-cluster-noise1.png)


```python
np.random.seed(1234)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.3, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.7))
```

    
![crescent-spectral-cluster-noise2.png](/images/spectral/crescent-spectral-cluster-noise2.png)

#### generate `make_circles()` -- the bull's eye data set:


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```

![bulls-eye.png](/images/spectral/bulls-eye.png)
    


There are two concentric circles. As before k-means will not do well here at all. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```


    
![kmean-bullseye.png](/images/spectral/kmean-bullseye.png)



```python
# apply the spectral_clustering() function
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```

![bullseye-spectral-cluster-40.png](/images/spectral/bullseye-spectral-cluster-40.png)
    



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.33))
```
![bullseye-spectral-cluster-33.png](/images/spectral/bullseye-spectral-cluster-33.png)
    



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.3))
```
![bullseye-spectral-cluster-30.png](/images/spectral/bullseye-spectral-cluster-30.png)



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.5))
```
![bullseye-spectral-cluster-50.png](/images/spectral/bullseye-spectral-cluster-50.png)



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.55))
```

![bullseye-spectral-cluster-55.png](/images/spectral/bullseye-spectral-cluster-5.png)
    


So I tried multiple `epsilon` here, from 0.3 to 0.55, the best value is around 0.33 - 0.5. Going out of this range would cause the clustering failed.


# References: 
**Professor Phil Chodrow notes in PIC16B Lectures and Blog Post 2 instruction.**
